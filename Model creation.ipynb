{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "809118fd-dea8-469b-b1af-ea022f9e004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39e04442-c7bb-492b-ac5d-67221b3f030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your dataset\n",
    "dataset_path = r\"F:\\Shakunthala\\Hope\\15. Lung Cancer detection + Mental health chatbot\\lung_image_sets\"\n",
    "cancer_folders = [\"adenocarcinoma\", \"squamous_cell_carcinoma\"]\n",
    "normal_folder = \"normal\"\n",
    "image_size = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "604a679d-1d79-42a0-ac37-7bef307586e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de51fe85-5994-4d9d-84b3-794b7a656a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cancer images → label = 1\n",
    "for folder_name in cancer_folders:\n",
    "    folder_path = os.path.join(dataset_path, folder_name)\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "            img_path = os.path.join(folder_path, file)\n",
    "            img = load_img(img_path, target_size=image_size)\n",
    "            img = img_to_array(img) / 255.0\n",
    "            data.append(img)\n",
    "            labels.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0f73e26-68d0-4d7d-bae1-8e60b85c5514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normal images → label = 0\n",
    "normal_path = os.path.join(dataset_path, normal_folder)\n",
    "for file in os.listdir(normal_path):\n",
    "    if file.endswith((\".jpg\", \".png\", \".jpeg\")):\n",
    "        img_path = os.path.join(normal_path, file)\n",
    "        img = load_img(img_path, target_size=image_size)\n",
    "        img = img_to_array(img) / 255.0\n",
    "        data.append(img)\n",
    "        labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d371a7-0431-458e-8256-25c8f62cea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to NumPy arrays\n",
    "data = np.array(data)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612a12f2-cb65-48c8-bc03-d33f20d1fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "175c805d-bad6-4b8b-9abf-ec5bdbd7f74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 89ms/step - accuracy: 0.9362 - loss: 0.1658 - val_accuracy: 0.9767 - val_loss: 0.0571\n",
      "Epoch 2/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 81ms/step - accuracy: 0.9780 - loss: 0.0661 - val_accuracy: 0.9847 - val_loss: 0.0352\n",
      "Epoch 3/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 81ms/step - accuracy: 0.9862 - loss: 0.0410 - val_accuracy: 0.9920 - val_loss: 0.0262\n",
      "Epoch 4/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - accuracy: 0.9909 - loss: 0.0269 - val_accuracy: 0.9910 - val_loss: 0.0217\n",
      "Epoch 5/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 79ms/step - accuracy: 0.9962 - loss: 0.0117 - val_accuracy: 0.9950 - val_loss: 0.0124\n",
      "Epoch 6/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 79ms/step - accuracy: 0.9971 - loss: 0.0079 - val_accuracy: 0.9970 - val_loss: 0.0102\n",
      "Epoch 7/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 80ms/step - accuracy: 0.9945 - loss: 0.0135 - val_accuracy: 0.9913 - val_loss: 0.0269\n",
      "Epoch 8/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 80ms/step - accuracy: 0.9956 - loss: 0.0100 - val_accuracy: 0.9947 - val_loss: 0.0144\n",
      "Epoch 9/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 79ms/step - accuracy: 0.9933 - loss: 0.0214 - val_accuracy: 0.9983 - val_loss: 0.0043\n",
      "Epoch 10/10\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 81ms/step - accuracy: 0.9993 - loss: 0.0023 - val_accuracy: 0.9907 - val_loss: 0.0278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x29f22889520>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e8803f6-b9b4-4650-8d29-cc6c4ba7ab1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to lung_cancer_app/detector/model.h5\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save(\"lung_cancer_app/detector/model.h5\")\n",
    "print(\"Model saved to lung_cancer_app/detector/model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b15e95-4a04-4996-8487-c2520cba7730",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
